안녕하세요. 반갑습니다.
딥러닝은 정말 흥미로운 분야에요.
그래서 저는 모델을 구현하면서 공부하고 있습니다.
이 논문의 제목은 'Attention Is All You Need' 입니다.
저자는 기존의 Seq2Seq 에서 사용되던 RNN 이나 CNN 을 사용하지 않았습니다.
대신에 FeedForward 와 Attention 으로 구성되어 있습니다.
그리고 문장의 위치를 입력값에 넣기 위해서 sinusodial 를 사용합니다.
핵심이 되는 Attention 은 아래와 같이 정리가 됩니다.
주의력 함수는 쿼리 및 키 값 쌍 집합을 출력에 매핑 하는 것으로 설명할 수 있습니다.
여기서 쿼리, 키, 값 및 출력은 모두 벡터입니다.
출력은 각 값에 할당된 가중치 합계가 해당 키와의 쿼리 호환성 기능에 의해 계산되는 값의 가중 합계로 계산됩니다.
여기서 헤드의 숫자를 늘리면 속도와 성능이 둘다 올라갑니다.
multi-head attention 을 통해 모델은 다양한 위치에서 서로 다른 표현 하위 영역의 정보에 공동으로 관심을 기울일 수 있습니다.
그리고 이 모델은 병렬화를 수행할 수 있으므로 교육 시간이 크게 단축됩니다.
구글 리서치 블로그에서는 '공동 참조 해상도'에 대해 좋은 성능 발휘 한다고 소개합니다.
이 코드들이 당신에게 도움이 되었으면 좋겠습니다.
이 저장소에 Star 주시는 것을 잊지마세요.
그리고 모델에 문제가 있거나, 개선할 부분이 있으면 언제든지 pull request 를 날려주세요.
한글에서 영어로 번역되는 이 작은 데이터셋은 여기까지 입니다.
데이터셋을 읽어주셔서 감사합니다.
좋은 하루 보내시길 바랍니다.